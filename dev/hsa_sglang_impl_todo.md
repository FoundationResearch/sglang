# HSA in SGLangï¼šè½åœ°ç‰ˆå®ç° TODOï¼ˆæŒ‰ä»£ç ç»“æ„ / æ–‡ä»¶ / ç±» / æ¥å£ï¼‰

æœ¬æ–‡æ¡£æ˜¯ `dev/hsa_dev_roadmap.md` çš„â€œå·¥ç¨‹è½åœ°ç‰ˆ TODOâ€ã€‚ç›®æ ‡æ˜¯æŠŠ HSA ä»¥ **`AttentionBackend`** çš„æ–¹å¼æ¥å…¥ SGLangï¼ˆç±»ä¼¼ `--attention-backend nsa`ï¼‰ï¼Œå¹¶ä¸¥æ ¼å¯¹é½ **Paged KVï¼ˆ`req_to_token` / `kv_indptr` / `kv_indices`ï¼‰** ä¸ **Radix prefix cache** çš„è¯­ä¹‰ã€‚

> é˜…è¯»é¡ºåºå»ºè®®ï¼šå…ˆçœ‹ `dev/nsa_in_sglang.md` å’Œ `python/sglang/srt/layers/attention/nsa_backend.py`ï¼Œç†è§£ SGLang çš„ metadata/indices ç»„ç»‡æ–¹å¼ï¼Œå†å›åˆ°è¿™é‡Œé€é¡¹è½åœ°ã€‚

---

## 0. å…ˆå†»ç»“çš„å¥‘çº¦ï¼ˆContractï¼‰

- **Chunk/Page è¯­ä¹‰**
  - **å¿…é¡»**ï¼š`chunk_size == page_size`ã€‚
  - **å¿…é¡»ï¼ˆLMK ä¸»çº¿ï¼‰**ï¼šæ¯ä¸ª page æœ€åä¸€ä¸ª slot å›ºå®šä¸º LMKï¼Œæ•…æ¯é¡µçœŸå® token ä¸Šé™ä¸º `page_size - 1`ã€‚
  - **å¿…é¡»ï¼ˆLMK ä¸»çº¿ï¼‰**ï¼šåªå¯¹ â€œcompleted pageâ€ï¼ˆLMK å·²å†™å…¥ KVï¼‰å®šä¹‰ \(E_i\) å¹¶å‚ä¸ selectionï¼›partial page ä¸è¿›å…¥å¯å¤ç”¨é›†åˆã€‚
- **\(E_i\) å­˜å‚¨ç²’åº¦**
  - **æ¨èä¸»çº¿ï¼ˆFlashHSA è¯­ä¹‰ï¼‰**ï¼š\(E_i\) ç”±è¯¥ page çš„ **LMK token çš„ K** å®šä¹‰ï¼š
    - `lmk_token_loc = page_id * page_size + (page_size - 1)`
    - selection ä» KV cache gather LMK-K å¾—åˆ° \(E_i\)ï¼ˆpaged-friendlyï¼‰
- **Selection è¾“å‡ºå¥‘çº¦**
  - è‡³å°‘å›ºå®šä¸€æ¡ä¸»çº¿ï¼ˆæ¨èï¼‰ï¼šè¾“å‡ºèƒ½ç›´æ¥é©±åŠ¨ paged kernel çš„ `selected_page_ids`/`page_table` + `weights`ã€‚
  - ä¸ NSA å¯¹é½ï¼šæœ€å¥½æ”¯æŒ fused transformï¼ŒæŠŠ topâ€‘k ç›´æ¥å˜æˆâ€œå¯ç”¨äº gather çš„ paged index è¡¨â€ã€‚
 - **LMK ä¸å¯è§è¾“å‡ºå¥‘çº¦**
   - æ¨ç†æ—¶ LMK token ä¸åº”è¢«é‡‡æ ·/ä¸åº”è¾“å‡ºç»™ç”¨æˆ·ï¼ˆä»…ç”¨äºå†™ KV ä¸äº§ç”Ÿ \(E_i\)ï¼‰ã€‚
   - `window_size`ï¼ˆSWAï¼‰æŒ‰ **åŒ…å« LMK çš„ token é•¿åº¦** è®¡æ•°ï¼ˆä¸ä½ å½“å‰çº¦å®šä¸€è‡´ï¼‰ã€‚

---

## 1. CLI / Registryï¼šæŠŠ `hsa` ä½œä¸º AttentionBackend æ¥å…¥

- **æ–‡ä»¶**ï¼š`python/sglang/srt/server_args.py`
  - **ä»»åŠ¡**ï¼š
    - å°† `"hsa"` åŠ å…¥ `ATTENTION_BACKEND_CHOICES`ï¼ˆä¸ `"nsa"` åŒçº§ï¼‰ã€‚
    - å¢åŠ  HSA ç›¸å…³å‚æ•°ï¼ˆå»ºè®®å…ˆæœ€å°é›†ï¼‰ï¼š
      - `--hsa-topk`
      - `--hsa-page-size`ï¼ˆæˆ–å¤ç”¨å…¨å±€ `--page-size` å¹¶åœ¨è¿è¡Œæ—¶ assertï¼‰
      - `--hsa-selection-strategy {group,head,softmax_head}`
      - `--hsa-layers`ï¼ˆä¾‹å¦‚ `0,2,4,...` æˆ– range è¯­æ³•ï¼›ä¹Ÿå¯å…ˆåš â€œall layersâ€ï¼‰
      - å¯é€‰ï¼š`--hsa-window-size` / `--hsa-enable-swa-fusion`
      - `--hsa-lmk-id`ï¼šLMK token idï¼ˆé»˜è®¤ -1 è¡¨ç¤ºç”¨ vocab_sizeï¼›å¯¹é½ FlashHSAï¼‰

- **æ–‡ä»¶**ï¼š`python/sglang/srt/layers/attention/attention_registry.py`
  - **ä»»åŠ¡**ï¼š
    - æ–°å¢ï¼š
      - `@register_attention_backend("hsa")`
      - `def create_hsa_backend(runner): return HSAAttnBackend(runner)`

**çŠ¶æ€**ï¼šâœ… å·²å®Œæˆï¼ˆCLI/Registry å·²æ¥å…¥ï¼Œå¯ç”¨ `--attention-backend hsa` é€‰æ‹©ï¼‰

---

## 2. Backendï¼šæ–°å¢ `HSAAttnBackend`ï¼ˆæ ¸å¿ƒè°ƒåº¦ç‚¹ï¼‰

- **æ–°å¢æ–‡ä»¶**ï¼š`python/sglang/srt/layers/attention/hsa_backend.py`
  - **æ–°å¢ç±»**ï¼š`class HSAAttnBackend(AttentionBackend)`
  - **éœ€è¦å®ç°çš„æ–¹æ³•ï¼ˆå¯¹é½ç°æœ‰ backend ä¹ æƒ¯ï¼‰**
    - `__init__(self, model_runner: ModelRunner, ...)`
      - è§£æï¼š`page_size`ã€`num_head/num_kv_head/head_dim`ã€deviceã€æ˜¯å¦ sliding windowã€‚
      - åˆå§‹åŒ–ï¼šå¯å¤ç”¨çš„ GPU bufferï¼ˆå‚è€ƒ `TritonAttnBackend` / `NativeSparseAttnBackend`ï¼‰ã€‚
    - `init_forward_metadata(self, forward_batch: ForwardBatch)`
      - è¯»å–ï¼š`forward_batch.req_pool_indices`ã€`forward_batch.seq_lens(_cpu)`ã€`forward_batch.req_to_token_pool.req_to_token`ã€‚
      - ç”Ÿæˆï¼š
        - `page_table_1`ï¼ˆtokenâ†’slot è¡¨ï¼Œpage_size=1 è¯­ä¹‰ï¼‰
        - `real_page_table`ï¼ˆpage_size>1 æ—¶çš„ page_id è¡¨ï¼Œå‚è€ƒ `NativeSparseAttnBackend._transform_table_1_to_real`ï¼‰
        - â€œå¯ç”¨äº selection çš„ chunk/page åˆ—è¡¨â€ï¼ˆä¾‹å¦‚æŒ‰ stride å–æ ·æ¯é¡µä¸€ä¸ª token_locï¼Œå† `//page_size` å¾— page_idï¼‰
      - ç¼“å­˜ï¼šå°† metadata å­˜åœ¨ `self.forward_metadata`ï¼ˆç±»ä¼¼ `TritonAttnBackend.forward_metadata`ï¼‰ã€‚
    - `forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache=True, **kwargs)`
      - å¦‚æœ `save_kv_cache`ï¼šè°ƒç”¨ `forward_batch.token_to_kv_pool.set_kv_buffer(...)` å†™ KVï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰ã€‚
      - è°ƒç”¨ selectionï¼ˆè§ç¬¬ 4 èŠ‚ï¼‰ï¼Œå¾—åˆ° `selected_page_ids`/indices + `weights`ã€‚
      - è°ƒç”¨ paged HSA decode kernelï¼ˆè§ç¬¬ 5 èŠ‚ï¼‰ï¼Œè¾“å‡º `o`ã€‚
    - `forward_prefill/forward_extend(...)`
      - å…ˆ correctnessï¼šå¯ä»¥å…ˆèµ°ç»„åˆç®—å­æˆ– referenceï¼ˆä½æ€§èƒ½ï¼‰é—­ç¯ï¼›
      - å†é€æ­¥æ›¿æ¢æˆ paged HSA prefill kernelã€‚
  - **æ–°å¢æ•°æ®ç»“æ„ï¼ˆå»ºè®®ä»¿ NSAï¼‰**
    - `@dataclass class HSAMetadata:`
      - `page_size`
      - `cache_seqlens_int32 / cu_seqlens_k`
      - `page_table_1 / real_page_table`
      - å¯é€‰ï¼š`page_table_1_flattened`ï¼ˆprefix sharing ä¼˜åŒ–éœ€è¦ï¼‰
      - å¯é€‰ï¼š`token_to_batch_idx` / `indexer_k_start_end`ï¼ˆè‹¥ selection éœ€è¦ ragged è®¿é—®ï¼‰
    - `@dataclass class HSAForwardMetadata:`ï¼ˆå¦‚éœ€æŠŠ selection / kernel è¾“å…¥ç¼“å­˜ä¸‹æ¥ï¼‰

**çŠ¶æ€**ï¼šğŸŸ¡ å·²éƒ¨åˆ†å®Œæˆï¼ˆPhaseâ€‘1 è°ƒåº¦ç‚¹å·²é—­ç¯ï¼›kernel æœªè½åœ°ï¼‰
- âœ… `HSAAttnBackend` å·²å­˜åœ¨ï¼Œå¹¶ä¸”å½“å‰é˜¶æ®µ **delegate åˆ° denseï¼ˆ`TritonAttnBackend`ï¼‰**ï¼Œå¯è·‘é€š end-to-end plumbing
- âœ… å·²å®ç°å¹¶ç¼“å­˜æœ€å° `HSAMetadata`ï¼ˆåŒ…å« `page_table_1` / `real_page_table` ä»¥åŠ `kv_indptr/kv_indices` æŒ‡é’ˆé€ä¼ ï¼‰
- âœ… å·²å®ç° decode selectionï¼ˆTorch referenceï¼‰ï¼Œå¹¶æŠŠç»“æœå†™å…¥ `HSAMetadata.hsa_*` debug å­—æ®µï¼ˆcompute ä» delegateï¼‰
- âœ… å·²åŠ å…¥ GPU-only smoke testï¼š`python/sglang/test/attention/test_hsa_backend_gpu.py`
- âœ… å·²åŠ å…¥ GPU-only çœŸå®é›†æˆæµ‹è¯•ï¼ˆä¸ monkeypatchï¼‰ï¼š`python/sglang/test/attention/test_hsa_backend_dense_integration_gpu.py`
- â³ æœªå®ç°ï¼špaged HSA kernelï¼ˆä»ç„¶èµ° dense attention è¾“å‡ºï¼‰

---

## 3. KV / \(E_i\)ï¼šä¸¥æ ¼å¯¹é½ FlashHSAï¼ˆLMK çœŸå® tokenï¼‰

- **æ–‡ä»¶**ï¼š`python/sglang/srt/mem_cache/memory_pool.py`
  - **çº¦æŸ**ï¼šä¸å¢åŠ ä»»ä½• per-page repr bufferã€‚
  - **\(E_i\) å®šä¹‰ï¼ˆFlashHSAï¼‰**ï¼š
    - æ¯ä¸ª page æœ€åä¸€ä¸ª slot ä¸º LMKï¼š`lmk_token_loc = page_id * page_size + (page_size - 1)`
    - æ¯å±‚ selection ä½¿ç”¨è¯¥å±‚ KV cache é‡Œçš„ `K[lmk_token_loc]` ä½œä¸º \(E_i\)
  - **å®‰å…¨æ€§**ï¼šé€šè¿‡ â€œcompleted-page gatingâ€ï¼ˆåªå…è®¸ `page_id < floor(seq_len / page_size)`ï¼‰é¿å…è¯»åˆ°æœªå®šä¹‰ KVã€‚

- **æ–‡ä»¶**ï¼š`python/sglang/srt/mem_cache/allocator.py`
  - **ä»»åŠ¡ï¼ˆåç»­å®‰å…¨å·¥ä½œï¼‰**ï¼šæŠŠ allocator/radix çš„ page reuse ç”Ÿå‘½å‘¨æœŸä¸ HSA çš„ completed-page è¯­ä¹‰å¼ºç»‘å®šï¼ˆç”Ÿäº§çº§å®‰å…¨ï¼‰ã€‚

- **æ–‡ä»¶**ï¼š`python/sglang/srt/mem_cache/swa_memory_pool.py`
  - **ä»»åŠ¡**ï¼šè‹¥ HSA éœ€è¦ SWA pool å‚ä¸ï¼ˆæ··å±‚/çª—å£ï¼‰ï¼Œæ˜ç¡® LMK åœ¨ full/swa çš„æ˜ å°„ç­–ç•¥ã€‚

**çŠ¶æ€**ï¼šâœ… å·²å®Œæˆï¼ˆéµå¾ª FlashHSAï¼›æ—  repr bufferï¼‰
- âœ… \(E_i\) æ¥è‡ª KV cache ä¸­ LMK token çš„ Kï¼ˆæŒ‰ layer åˆ†åˆ« gatherï¼‰
- âœ… é€šè¿‡ completed-page gating é¿å…è¯»åˆ°æœªå®šä¹‰ KV

---

## 4. Selection / Topâ€‘Kï¼šä» \(q \cdot E_i\) åˆ° `selected_page_ids + weights`

> ç›®æ ‡ï¼šselection å¿…é¡»æ˜¯ â€œpaged-friendlyâ€ çš„è¾“å‡ºï¼Œé¿å…åœ¨ Python ä¾§åšå¤§é‡ gather/transformã€‚

- **æœ¬é˜¶æ®µå®ç°ç­–ç•¥ï¼ˆå…ˆ SWAâ†’HSAï¼Œéèåˆï¼›ä½†ä¿ç•™æœªæ¥èåˆçš„çµæ´»æ€§ï¼‰**
  - **å›ºå®š K**ï¼š`--hsa-topk` å›ºå®šï¼Œè¾“å‡ºä¸è¶³æ—¶ç”¨ `-1` paddingï¼Œå¹¶é…å¥— mask / `-inf` scoreã€‚
  - **å€™é€‰é›† = æœ¬æ¬¡ query çš„æ´»è·ƒ pages**ï¼šä»…å¯¹ `req_to_token[:seq_len]` ä¸­å‡ºç°è¿‡çš„ `page_id` è®¡ç®— \(q \cdot E_i\)ã€‚
  - **SWAâ†’HSA æ¨¡å¼çš„ SWA æ’é™¤**ï¼šå…ˆç”¨ SWA è¦†ç›–â€œè¿‘é‚»çª—å£â€ï¼Œselection åªåœ¨çª—å£ä¹‹å¤–çš„ pages ä¸Šåš topâ€‘kï¼ˆç­‰ä»·äºåŸ repo çš„ â€œcausal block maskâ€ æ€è·¯ï¼‰ã€‚
  - **completed-page gating**ï¼šselection å¿…é¡»æ’é™¤æœªå®Œæˆ pagesï¼ˆLMK æœªå†™å…¥ KV çš„ pagesï¼‰ï¼Œé¿å…è¯»åˆ°æœªå®šä¹‰ KVã€‚
  - **ç­–ç•¥æ”¯æŒ**ï¼šå…ˆå®ç° `group`/`head`ï¼ˆå‘½åå¯¹é½ `dev/hsa-kernel-main`ï¼‰ï¼Œæœªæ¥å†åŠ  `softmax_head`ï¼ˆSWA/HSA èåˆéœ€è¦ `lse_swa`ï¼‰ã€‚

- **æ–°å¢ç›®å½•ï¼ˆå»ºè®®ï¼‰**ï¼š`python/sglang/srt/layers/attention/hsa/`
  - **æ–°å¢æ–‡ä»¶**ï¼š`selector.py`
    - `class HSASelector:`ï¼ˆæˆ–å‡½æ•°é›†ï¼‰
      - `def compute_logits(q, chunk_repr, *, strategy, ...) -> logits`
      - `def topk_transform(logits, topk, *, page_table_1/real_page_table, ...) -> selected`
    - **å¯¹é½ NSA çš„æ€è·¯**
      - å‚è€ƒ `python/sglang/srt/layers/attention/nsa_backend.py`ï¼š
        - `TopkTransformMethod.PAGED`
        - `topk_transform()` çš„ fused transformï¼ˆé¿å…é¢å¤– gatherï¼‰

- **ä» dev/hsa-kernel-main è¿ç§»ï¼ˆå¯é€‰è·¯å¾„ï¼‰**
  - å°† `dev/hsa-kernel-main/ops/topk_*.py` ä¸­å¯å¤ç”¨çš„ kernel è¿ç§»/é‡å†™åˆ°ï¼š
    - `python/sglang/srt/layers/attention/hsa/kernels/topk_*.py`
  - å…³é”®æ”¹é€ ç‚¹ï¼šè¾“å…¥å¿…é¡»æ”¯æŒ `page_id`/paged è¡¨ï¼Œè€Œä¸æ˜¯å‡è®¾ landmarks è¿ç»­ã€‚

**çŠ¶æ€**ï¼šğŸŸ¡ å®ç°ä¸­ï¼ˆå·²é—­ç¯ decode çš„ Torch reference selection + CUDA å•æµ‹ï¼‰
- âœ… æ–°å¢ï¼š`python/sglang/srt/layers/attention/hsa/selector.py`
  - `build_active_page_candidates(...)`ï¼šæ´»è·ƒ pages + SWA window pages æ’é™¤
  - `select_topk_pages_decode(...)`ï¼šdecode topâ€‘kï¼ˆå›ºå®š Kï¼›`group/head`ï¼‰
- âœ… `HSAAttnBackend.forward_decode` å·²è¿è¡Œ selectionï¼Œå¹¶æŠŠç»“æœå†™åˆ° `HSAMetadata` çš„ debug å­—æ®µï¼ˆcompute ä» delegate denseï¼‰
- âœ… GPU-only å•æµ‹ï¼š`python/sglang/test/attention/test_hsa_selector_decode_gpu.py`

---

## 4.x LMK runtime æ³¨å…¥ï¼ˆä¸å¯è§è¾“å‡ºé—­ç¯ï¼‰

- **ç›®æ ‡**ï¼šè®© LMK æˆä¸º engine-visible tokenï¼ˆå†™ KVã€è¿› prefix/radixï¼‰ï¼Œä½†å¯¹ç”¨æˆ·ä¸å¯è§ã€‚
- **è½åœ°æ–‡ä»¶**
  - `python/sglang/srt/managers/scheduler.py`
    - å¯¹ `--attention-backend hsa` çš„è¯·æ±‚å¯ç”¨ `Req.enable_hsa_lmk(page_size, lmk_id)`
  - `python/sglang/srt/managers/schedule_batch.py`
    - `Req` æ”¯æŒï¼š
      - prompt æ’å…¥ï¼šæ¯ `page_size-1` ä¸ª token æ’å…¥ 1 ä¸ª LMKï¼ˆFlashHSA è¯­ä¹‰ï¼‰
      - decode å¼ºåˆ¶ï¼šå½“ `len(fill_ids) % page_size == page_size-1` æ—¶ï¼Œä¸‹ä¸€æ­¥å¼ºåˆ¶æ’å…¥ LMKï¼ˆä¸å¯è§ï¼‰
  - `python/sglang/srt/managers/scheduler_output_processor_mixin.py`
    - decode è¾“å‡ºå¤„ç†ï¼šLMK åªè¿› `fill_ids`ï¼Œä¸è¿› `output_ids`ï¼Œå› æ­¤ä¸è¢« stream/detokenizer çœ‹åˆ°
  - `python/sglang/srt/model_executor/model_runner.py`
    - sampling æ—¶æ°¸è¿œ mask æ‰ LMK idï¼Œé¿å…å…¶è¢«é‡‡æ ·ä¸ºç”¨æˆ· token
- **GPU-only tests**
  - `python/sglang/test/attention/test_hsa_lmk_runtime_injection_gpu.py`

**çŠ¶æ€**ï¼šâœ… å·²å®Œæˆ

---

## 5. Kernelï¼šPaged HSAï¼ˆdecode å…ˆé—­ç¯ï¼‰

- **æ–°å¢ç›®å½•ï¼ˆå»ºè®®ï¼‰**ï¼š`python/sglang/srt/layers/attention/triton_ops/hsa/`
  - **æ–°å¢æ–‡ä»¶**ï¼š`decode_hsa.py`
    - `def decode_hsa_fwd(Q, K_Buffer, V_Buffer, kv_indptr, kv_indices, selected_page_ids/indices, weights, ...) -> O`
    - **ç¡¬çº¦æŸ**ï¼šK/V load å¿…é¡»é€šè¿‡ `kv_indices` æˆ– `page_id` å±•å¼€é—´æ¥å¯»å€ï¼Œä¸èƒ½ç”¨ `blk_idx * block_size` è¿ç»­å‡è®¾ã€‚
    - æ¥å£é£æ ¼å°½é‡è´´è¿‘ `triton_ops/decode_attention.py`ï¼Œæ–¹ä¾¿å¤ç”¨ buffer / cuda graph ç»éªŒã€‚
  - **æ–°å¢æ–‡ä»¶ï¼ˆåç»­ï¼‰**ï¼š`prefill_hsa.py` / `extend_hsa.py`

- **ä» dev/hsa-kernel-main è¿ç§»**
  - `dev/hsa-kernel-main/ops/hsa_head_decode.py` / `hsa_head_prefill.py`
    - ç°çŠ¶åå‘ â€œè¿ç»­ KV + block_idx * block_sizeâ€ï¼›
    - éœ€è¦é‡å†™ load é€»è¾‘ä¸º pagedï¼ˆå‚è€ƒ `decode_attention.py` çš„ `kv_loc` loadï¼‰ã€‚

---

## 6. æµ‹è¯•ï¼šæ”¾åœ¨ SGLang ç°æœ‰æµ‹è¯•ç»“æ„é‡Œï¼ˆ`python/sglang/test/attention/`ï¼‰

- **æ–°å¢æµ‹è¯•æ–‡ä»¶ï¼ˆå»ºè®®æœ€å°é›†ï¼‰**
  - `python/sglang/test/attention/test_hsa_contract.py`
    - page_size/chunk_size å¥‘çº¦ã€partial page è§„åˆ™ã€page_id æ˜ å°„ä¸€è‡´æ€§
  - `python/sglang/test/attention/test_hsa_paged_kernel.py`
    - æ„é€ ç¦»æ•£ `kv_indices`ï¼ŒéªŒè¯ kernel è¯»å–æ­£ç¡®
  - `python/sglang/test/attention/test_hsa_backend_decode.py`
    - ä¸ dense decode å¯¹ç…§ï¼ˆæˆ– referenceï¼‰éªŒè¯ correctness

**å½“å‰å·²æœ‰æµ‹è¯•**
- âœ… `python/sglang/test/attention/test_hsa_backend_gpu.py`ï¼ˆGPU-onlyï¼Œsmokeï¼šå¯è·‘ + delegateï¼‰
- âœ… `python/sglang/test/attention/test_hsa_backend_dense_integration_gpu.py`ï¼ˆGPU-onlyï¼šçœŸå® Triton é›†æˆ + selection å¯è·‘ï¼‰
- âœ… `python/sglang/test/attention/test_hsa_lmk_runtime_injection_gpu.py`ï¼ˆGPU-onlyï¼šLMK prompt æ’å…¥ + decode å¼ºåˆ¶ LMK ä¸”ä¸å¯è§ï¼‰
- âœ… `python/sglang/test/attention/test_hsa_selector_decode_gpu.py`ï¼ˆGPU-onlyï¼šselection correctnessï¼‰

**ä»ç¼ºçš„æµ‹è¯•ï¼ˆå»ºè®®æŒ‰ä¼˜å…ˆçº§ï¼‰**
- **P1**ï¼šçœŸæ­£çš„ç«¯åˆ°ç«¯ï¼šscheduler è·‘å‡ è½® decodeï¼ŒéªŒè¯è¾“å‡º token åºåˆ—ä¸­ä¸å‡ºç° LMKï¼Œä½†å†…éƒ¨ seqlen/kv/cache æŒç»­å¢é•¿ï¼ˆå« radix prefix å‘½ä¸­åœºæ™¯ï¼‰
- **P2**ï¼šextend/prefill çš„ LMK è‡ªåŠ¨æ’å…¥ï¼ˆraggedï¼‰ä¸ prefix/radix å¯¹é½æµ‹è¯•
- **P3**ï¼šCUDA graph / speculative / overlap / sliding window çš„æ”¯æŒçŸ©é˜µæµ‹è¯•ï¼ˆå…ˆå†™ skip/xfail ä¹Ÿå¯ä»¥ï¼‰

---

## 7. æ€§èƒ½ä¸å…¼å®¹æ€§ï¼ˆä¸Šçº¿å‰å¿…é¡»æ˜ç¡®çš„æ”¯æŒçŸ©é˜µï¼‰

- **CUDA graph**
  - éœ€è¦æ˜ç¡®ï¼šHSA backend æ˜¯å¦æ”¯æŒ decode CUDA graphï¼›è‹¥æ”¯æŒï¼Œå“ªäº› buffer éœ€è¦ â€œstatic shapes + preallocatedâ€ã€‚
- **Speculative decoding**
  - éœ€è¦æ˜ç¡®ï¼štopk>1 ä¸ page_size>1 çš„ç»„åˆåœ¨æŸäº›åç«¯å­˜åœ¨ä¸ç¨³å®šæ€§ï¼ˆSGLang é‡Œå·²æœ‰ç›¸å…³ guardï¼‰ï¼›HSA çš„æ”¯æŒç­–ç•¥è¦å…ˆå†™æ¸…ã€‚
- **Sliding window / SWA**
  - éœ€è¦æ˜ç¡®ï¼šHSA æ˜¯å¦å’Œ sliding window attention å…±å­˜ï¼›è‹¥å…±å­˜ï¼Œselection çš„ mask ä¸ window çš„èåˆæ–¹å¼è¦å›ºå®šã€‚
- **é‡åŒ– KVï¼ˆfp8/fp4ï¼‰**
  - å…ˆå®šä¹‰æ”¯æŒçŸ©é˜µï¼šHSA æ˜¯å¦éœ€è¦å…ˆæ”¯æŒ fp16/bf16ï¼Œå†é€æ­¥æ‰©å±•åˆ°é‡åŒ–ç¼“å­˜ã€‚


